\documentclass[runningheads]{llncs}
\synctex=1
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{mathpartir}
\usepackage{cite}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{microtype}
\usepackage[inline]{enumitem}
\usepackage{color}

\input{../macros.tex}

\crefname{question}{question}{questions}

\begin{document}

\title{The Concurrent Calculi Formalisation Benchmark\thanks{The work is partially supported by EPSRC EP/T006544/2.}}

\author{
     Marco Carbone \inst{1}\orcidID{0000-0001-9479-2632}
\and David Castro-Perez \inst{2}\orcidID{0000-0002-6939-4189}
\and Francisco Ferreira \inst{3}\orcidID{0000-0001-8494-7696}
\and Lorenzo Gheri \inst{4}\orcidID{0000-0002-3191-7722}
\and Frederik Krogsdal Jacobsen \inst{5}\orcidID{0000-0003-3651-8314}
\and Alberto Momigliano \inst{6}\orcidID{0000-0003-0942-4777}
\and Luca Padovani \inst{7}\orcidID{0000-0001-9097-1297}
\and Alceste Scalas \inst{5}\orcidID{0000-0002-1153-6164}
\and Dawit Tirore \inst{1}\orcidID{0000-0002-1997-5161}
\and Martin Vassor \inst{8}\orcidID{0000-0002-2057-0495}
\and Nobuko Yoshida \inst{8}\orcidID{0000-0002-3925-8557}
\and Daniel Zackon \inst{9}\orcidID{0009-0008-6153-2955}
}

\institute{
     IT University of Copenhagen, Copenhagen, Denmark \email{maca@itu.dk}, \email{dati@itu.dk}
\and University of Kent, Canterbury, United Kingdom \email{d.castro-perez@kent.ac.uk}
\and Royal Holloway, University of London, Egham, United Kingdom \email{francisco.ferreiraruiz@rhul.ac.uk}
\and University of Liverpool, Liverpool, United Kingdom \email{lorenzo.gheri@liverpool.ac.uk}
\and Technical University of Denmark, Kgs. Lyngby, Denmark \email{fkjac@dtu.dk}, \email{alcsc@dtu.dk}
\and Università degli Studi di Milano, Milan, Italy \email{momigliano@di.unimi.it}
\and Università di Camerino, Camerino, Italy \email{luca.padovani@unicam.it}
\and University of Oxford, Oxford, United Kingdom \email{martin.vassor@cs.ox.ac.uk}, \email{nobuko.yoshida@cs.ox.ac.uk}
\and McGill University, Montreal, Canada \email{daniel.zackon@mcgill.ca}
}

\authorrunning{M. Carbone et al.}

\maketitle

\begin{abstract}
  POPLMark and POPLMark Reloaded sparked a flurry of work on
  machine-checked proofs, and fostered the adoption of proof
  mechanisation in programming language research.  Both challenges
  were purposely limited in scope, and they do not address
  concurrency-related issues.  We propose a new collection of
  benchmark challenges focused on the difficulties that typically
  arise when mechanising formal models of concurrent and distributed
  programming languages, such as process calculi.  Our benchmark
  challenges address three key topics: linearity, scope extrusion, and
  coinductive reasoning.  The goal of this new benchmark is to
  clarify, compare, and advance the state of the art, fostering the
  adoption of proof mechanisation in future research on concurrency.

\keywords{Mechanisation \and Process calculi \and Benchmark \and Linearity \and Scope extrusion \and Coinduction}
\end{abstract}

\section{Introduction}
The POPLMark challenge~\cite{POPLMark}
played a pivotal role in advancing the field of proof assistants,
libraries, and best practices for the mechanisation of programming language research. By providing a shared framework for systematically
evaluating mechanisation techniques, it catalysed a significant
shift towards publications that include mechanised proofs within the programming language research community.
POPLMark Reloaded~\cite{POPLMarkReloaded} introduced a similar programme for proofs using logical relations.
These initiatives had a narrow focus, and their authors recognised the importance of addressing topics such as coinduction and linearity in the future.

In this spirit, we introduce a new collection of benchmarks 
crafted to tackle common challenges encountered during
the mechanisation of formal models of concurrent and distributed
programming languages.  We focus on process calculi, as
they provide a simple but realistic showcase of these challenges.  Concurrent calculi are
notably subtle: for instance, it took some years before an incorrect
subject reduction proof in the original paper on session
subtyping~\cite{GH99} was discovered and then rectified in the extended journal version~\cite{GH05} with the use of
polarities.
Similarly, other key results in papers on session types have subsequently
been proven incorrect~\cite{Gay2020,10.1145/3290343}, demonstrating the
need for machine-checked proofs.

While results about concurrent
formalisms have already been mechanised (as we will discuss further
below), our experience is that choosing appropriate mechanisation
techniques and tools remains a significant challenge and that their
trade-offs are not well understood.  This often leads researchers
toward a trial-and-error approach, resulting in sub-optimal solutions,
wasted mechanisation efforts, and techniques and results that are hard
to reuse. For example, Cruz-Filipe et al.~\cite{Cruz-Filipe2021b} note that the high-level parts of
mechanised proofs closely resemble the informal ones, while the
main challenge lies in getting the infrastructure right.

Our benchmark challenges (detailed in \cref{app:challenges} and on our website) consider
\emph{in isolation} three key aspects that frequently pose difficulties
when mechanising concurrency theory, which we will discuss in more detail in the next section: \emph{linearity}, \emph{scope
  extrusion}, and \emph{coinductive reasoning}.
Mechanisations must often address several of these aspects at the same time; however, we
see the combination of techniques as a next
step, as discussed in \cref{sec:going-beyond}.

We have begun collecting solutions to our challenges on our website:
\begin{center}
  \url{https://concurrentbenchmark.github.io/}
\end{center}
We intend to use the website to promote best practices and tutorials derived from solutions to our challenges.
We encourage readers to try the challenges using their favourite techniques, and to send us their solutions and experience reports.

\section{Overview and Design Considerations}\label{sec:design-discussion}
In this section, we outline the factors considered when designing the benchmark challenges.
We begin with some general remarks, then describe the individual design considerations for each challenge, and the criteria for evaluating solutions.

Similarly to the authors of POPLMark, we seek to answer several questions:
\begin{enumerate}[label=\textbf{(Q\arabic*)},leftmargin=10mm]
\item\label[question]{item:rq1} What is the current state of the art in the mechanisation of  the meta-theory of process calculi?
\item\label[question]{item:rq2} Which techniques and best practices can be recommended when starting new mechanisation projects involving process calculi?
\item\label[question]{item:rq3} What improvements are needed to make mechanisation tools more user-friendly with regard to the issues faced when mechanising process calculi?
\end{enumerate}

To provide a framework in which to answer these questions, our benchmark is designed to satisfy three main design goals:
\begin{enumerate}[label=\textbf{(G\arabic*)},leftmargin=10mm]
\item\label{item:goal-comperison-accessibility} To enable the comparison of
  proof mechanisation approaches by making the challenges accessible to
  mechanisation experts who may be unfamiliar with concurrency theory;

\item\label{item:goal-tutorials} To encourage the development of guidelines and
  tutorials demonstrating and comparing existing proof mechanisation
  techniques, libraries, and proof assistant features; and

\item\label{item:goal-reusability} To prioritise the exploration of mechanisation
  techniques that are reusable for future research.
\end{enumerate}
We also aim at strengthening the culture of mechanisation, by rallying the community to collaborate on exploring and developing new tools and techniques.

To achieve design goal~\ref{item:goal-comperison-accessibility}, our challenges explore the three aspects
(linearity, scope extrusion, coinduction) independently, so that they
may be solved individually and in any order; each challenge is small and easily understandable with basic knowledge of
textbook concurrency theory, process calculi, and type theory.  The
process calculi used in the challenges focus on the features that
we want to emphasise, and omit all constructs that would complicate the mechanisation without bringing
tangible insights.  The minimality and uniformity of the
calculi also allows us to target design goal~\ref{item:goal-tutorials}.
Aligned with design goal~\ref{item:goal-reusability}, our challenges concern the fundamental meta-theory of process calculi.
Our challenges centre around well-established results, showcasing proof techniques that can be leveraged in many applications (as we will further discuss in \cref{sec:going-beyond}).

\subsection{Linearity}
Linear typing systems allow for the tracking of resource usage in a program. In
the case of typed (in particular, session-typed) process calculi, linearity is
widely used for checking if and how a channel is used to send or receive values.
This substructurality~\cite[Ch. 1]{Pierce2004-oq} gives rise to mechanisation difficulties: \eg deciding how to \emph{split the
typing context} in a parallel composition.

The goal of our challenge on linear reasoning is to prove a type safety theorem
for a process calculus with session types, by combining subject
reduction with the absence of errors.  For simplicity we model only linear (as
opposed to shared) channels. Inspired by Vasconcelos~\cite{Vasconcelos2012}, we
define a syntax where a restriction $(\bm{\nu} ab)$ binds two dual names $a$
and $b$ as opposite endpoints of the same channel; their duality is
reflected in the type system. We model a simple notion of error: well-typed
processes must never use dual channel endpoints in a non-dual way (\eg by
performing concurrent send/receive operations on the same endpoint, or two
concurrent send operations on dual endpoints).
The operational semantics is a standard reduction relation. Proving subject reduction thus requires proving type preservation for structural congruence.

We designed this challenge to focus on linear reasoning while minimising
definitions and other concerns.  We therefore forgo name passing:
send/receive operations only support values that do not include channel names, so
the topology of the communication network described by a process cannot change.
We do not allow recursion or replication, hence infinite behaviours cannot be
expressed. We also forgo more sophisticated notions of error-freedom (\eg
deadlock freedom) as proving them would distract from the core linear
aspects of the challenge.

In mechanised meta-theory, addressing linearity means choosing an appropriate
representation of a linear context.  While the latter  is perhaps
best seen as a multiset, most proof assistants have better support for lists.
This representation is intuitive, but may require
establishing a large number of technical lemmata that are orthogonal to the
problem under study (in our case, proving type safety for session types).
Several designs are possible: one can label
occurrences of resources to constrain their usage (\eg~\cite{CicconeP20}), or impose a multiset structure over lists
(\eg~\cite{Danielsson12,ChaudhuriLR19}). Alternatively, contexts can be
implemented as finite maps (as in~\cite{Castro2020}), whose operations are
sensitive to a linear discipline. In all these cases, the effort required to
develop the infrastructure is significant.
One alternative strategy is to bypass the problem of context splitting
by adopting ideas from algorithmic linear type checking. One such
approach, known as ``typing with leftovers,'' is exemplified
in~\cite{DBLP:conf/forte/ZalakainD21}.
Similarly, context splitting can be eliminated by delegating linearity checks to a
\emph{linear predicate} defined on the process structure (\eg~\cite{BP23}).
These checks serve as additional conditions within the typing rules.
Whatever the choice, list-based encodings can be refined to be intrinsically-typed if the
proof assistant supports dependent types
(see~\cite{Thiemann2019,CicconeP20,RouvoetPKV20}).

A radically different approach is to adopt a \emph{substructural}
meta-logical framework, which handles resource
distribution implicitly, including splitting and
substitution: users need only map their linear operations to the
ones offered by the framework.  The only such framework is
\emph{Celf}~\cite{Schack-Nielsen:IJCAR08} (see the encoding of session
types in~\cite{Bock2016}); unfortunately, \emph{Celf} does not yet
fully support the verification of meta-theoretic properties.  A
compromise is the \emph{two-level} approach, \ie encoding a
substructural specification logic in a mainstream proof assistant and
then using that logic to state and prove linear properties (for a
recent example, see~\cite{Felty:MSCS21}). 

\subsection{Scope Extrusion}
This challenge revolves around the mechanisation of scope
extrusion, by which a process can send restricted names to another
process, as long as the restriction can safely be extruded to include
the receiving process.  The setting for this challenge is a
``classic'' untyped \( \pi \)-calculus, where (unlike the calculi in
the other challenges) names can be sent and received, and bound by
input constructs.  We define two different semantics for our system:
\begin{enumerate}
\item A reduction system: this avoids explicit reasoning about scope
  extrusion by using structural congruence, allowing
  implementors to explore different ways to encode the latter (\eg
  via process contexts or compatible refinement);
\item An (early) labelled transition system.
\end{enumerate}
The goal of our challenge on scope extrusion is to prove that the two semantics are
equivalent up to structural congruence.

This is the challenge most closely related to POPLMark,
as it concerns the properties of binders, whose encoding has been
extensively studied with respect to $\lambda$-calculi. Process calculi
present additional challenges, typically including several
different binding constructs: inputs bind a received name or value,
recursive processes bind recursion variables, and restrictions bind
names. The first two act similarly to the binders in
$\lambda$-calculi, but restrictions may be more challenging due to
scope extrusion.
Scope extrusion requires reasoning about free variables, so approaches that identify \(\alpha\)-equivalent processes cannot be directly applied.

Given those peculiarities, the syntax and semantics of $\pi$-calculi
have been mechanised from an early age (see~\cite{Melham1994}) with
many proof assistants and in many encoding styles.  Despite this,
almost all of these mechanisations rely on ad-hoc solutions to encode
scope extrusion.  They range from concrete encodings based on named
syntax~\cite{Melham1994} to basic de Bruijn~\cite{Hirschkoff1997,
  Perera2018} and locally-nameless representation~\cite{Castro2020}.
Nominal approaches are also common (see~\cite{Bengtson2009}), but they
may be problematic in proof assistants based on constructive type
theories.  An overall comparison is still lacking, but the case
study~\cite{AmbalLS21} explores four approaches to encoding binders in
Coq in the context of higher-order process calculi.  The authors report that working directly
with de~Bruijn indices was easiest since the approaches
developed for $\lambda$-calculus binders worked poorly with scope
extrusion.

Higher-order abstract syntax (HOAS) has seen extensive
use in formal reasoning in this area~\cite{Despeyroux2000,Honsell2001,Tiu2010,ChaudhuriCM15,Castro-Perez2021}. Its
weak form aligns well with mainstream inductive proof assistants,
significantly simplifying the encoding of typing systems and operational
semantics. However, when addressing more intricate concepts like
bisimulation, extensions to HOAS are needed. These extensions may take
the form of additional axioms~\cite{Honsell2001} or require niche proof assistants
such as Abella, which features a special quantifier for handling
properties related to names~\cite{GacekMN11}.

\subsection{Coinduction}

Process calculi typically include constructs that allow processes to
adopt infinite behaviours.  Coinduction serves as a fundamental method
for the definition and analysis of infinite objects, enabling the
examination of their behaviours.

The goal of our challenge on coinductive reasoning is to prove that
\emph{strong barbed bisimilarity} can be turned into a congruence by
making it sensitive to substitution and parallel composition. The crux
of our challenge is the effective use of coinductive up-to techniques.
The intention is that the result should be relatively easy to
achieve once the main properties of bisimilarity are established.
% should be easy to use the bisimulation relation and its properties,
% but this may be difficult with some mechanisation approaches.

The setting for our challenge is an untyped \(\pi\)-calculus augmented
with process replication in order to enable infinite behaviours.  We
do not include name passing since it is orthogonal to our aim of
exploring coinductive proof techniques.  We base our definition of
bisimilarity on a labelled transition system semantics and an
observability predicate describing the communication steps available
to a process.  The description of strong barbed bisimulation is one of
the first steps when studying the behaviour of process calculi, both
in textbooks (\eg~\cite{picalcbook}) and in existing mechanisations.
Though weak barbed congruence is a more common behavioural
equivalence, we prefer strong equivalences to simplify the theory by
avoiding the need to abstract over the number of internal transitions
in a trace.

% AM <-- review
While many proof assistants support coinductive techniques, they do so
through different formalisms.
Some systems even offer multiple abstractions for utilising coinduction.
For instance, Agda offers musical notation, co-patterns and productivity checking via sized types~\cite{Abel2013};
Coq features guarded recursion and refined fixed point approaches via libraries for \eg parameterised coinduction~\cite{Hur2013}, coinduction up-to~\cite{Pous16} and interaction trees~\cite{Xia2019}.

When reasoning over bisimilarity many authors rely on the native
coinduction offered by the chosen proof
assistant~\cite{Bengtson2016,Kahsai2008,Thiemann2019,Gay2020}, while
others prefer a more ``set-theoretic'' approach~\cite{Hirschkoff1997,Bengtson2009,Maksimovic2015,Pohjola2022}. Some
use both and establish an internal adequacy~\cite{Honsell2001}. Few
extend the proof assistant foundations to allow, \eg, reasoning about
bisimilarity up-to~\cite{ChaudhuriCM15}.

\subsection{Evaluation Criteria}
The motivation behind our benchmark is to obtain evidence towards answering
\cref{item:rq1,item:rq2,item:rq3}. We are therefore
interested not only in the solutions, but also in the experience of
solving the challenges with the chosen approach.  Solutions to our
challenges should be compared on three axes:
\begin{enumerate}
\item Mechanisation overhead: the amount of manually-written infrastructure and setup needed to express the definitions in the mechanisation;
\item Adequacy of the formal statements in the mechanisation: whether the proven theorems are easily recognisable as the theorems from the challenge; and
\item Cost of entry for the tools and techniques employed: the difficulty of learning to use the techniques.
\end{enumerate}
Solutions to our challenges need not strictly follow the definitions and lemmata set out in the challenge text, but solutions which deviate from the original challenges should present more elaborate argumentation for their adequacy.

\section{Future Work and Conclusions}\label{sec:going-beyond}
Our benchmark challenges do not cover all issues in the field, but focus on the fundamental aspects of linearity, scope extrusion, and coinduction.
Many mechanisations need to combine techniques to handle several of these aspects, and some may also need to handle aspects that are not covered by our benchmark.

Combining techniques for mechanising the fundamental aspects covered in our benchmark is non-trivial.
While we focus on the aspects individually to simplify the challenges, we are also interested in exploring how techniques interact.
%Some authors have already described the difficulties that combining techniques may induce in a mechanisation, \eg~\cite{DBLP:conf/forte/ZalakainD21}.\todo{Add more references here?}

Much current research on concurrent calculi includes aspects that are
not covered by our benchmark challenges, for example constructs such as
choice constructs and recursion.
%  Features that are in wide
% use, but not covered by our benchmark, include choice constructs and
% recursion. 
Some interesting research topics that build on the
fundamental aspects in our benchmark include multiparty session
types~\cite{10.1145/2827695}, choreographies~\cite{Carbone2013},
higher-order calculi~\cite{Hirsch2022}, conversation
types~\cite{DBLP:journals/tcs/CairesV10}, psi-calculi~\cite{lmcs:696},
and encodings between different
calculi~\cite{DBLP:journals/iandc/Gorla10,DBLP:journals/iandc/DardhaGS17}.
The meta-theory of these topics includes aspects---\eg well-formedness conditions on
global types, partiality of end-point projection functions,
\etc---that we do not address.

Our coinduction challenge only addresses two notions of process equivalence, but many more exist in the literature.
Coinduction may also play a role in recursive processes and session types: recursive session types can be expressed in \emph{infinitary form} by interpreting their typing rules coinductively~\cite{DerakhshanPfenning22,HornePadovani23}.

Unlike POPLMark, we consider \emph{animation} of calculi (as in~\cite{Castro-Perez2021}) out of scope for our benchmark.
Finally, our challenges encourage, but do not require, exploring proof automation, as offered by \eg the \emph{Hammer} protocol~\cite{BohmeN10,CzajkaK18}.

Ultimately, the fundamental aspects covered by our benchmark serve as the building blocks for most current research on concurrent calculi.
It is our hope and aim that exploring and comparing solutions to our challenges will move the community closer to a future where the key basic proof techniques for concurrent calculi are as easy to mechanise as they are to write on paper.

\bibliographystyle{splncs04}
\bibliography{../references}

\clearpage
\appendix
\section{Challenges}\label{app:challenges}
\input{../challenges/challenges.tex}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
