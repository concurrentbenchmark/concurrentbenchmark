\documentclass[runningheads]{llncs}
\synctex=1
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{mathpartir}
\usepackage{cite}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage[inline]{enumitem}
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[status=draft]{fixme}
\fxsetup{theme=color}
% \setlength{\textwidth}{12.4cm}
\setlength{\textheight}{19.9cm}

\input{../macros.tex}

\begin{document}

\title{The Concurrent Calculi Formalisation Benchmark}

\author{
     Marco Carbone \inst{1}\orcidID{0000-0001-9479-2632}
\and David Castro-Perez \inst{2}\orcidID{0000-0002-6939-4189}
\and Francisco Ferreira \inst{3}\orcidID{0000-0001-8494-7696}
\and Lorenzo Gheri \inst{4}\orcidID{0000-0002-3191-7722}
\and Frederik Krogsdal Jacobsen \inst{5}\orcidID{0000-0003-3651-8314}
\and Alberto Momigliano \inst{6}\orcidID{0000-0003-0942-4777}
\and Luca Padovani \inst{7}\orcidID{0000-0001-9097-1297}
\and Alceste Scalas \inst{5}\orcidID{0000-0002-1153-6164}
\and Dawit Tirore \inst{1}\orcidID{0000-0002-1997-5161}
\and Martin Vassor \inst{8}\orcidID{0000-0002-2057-0495}
\and Nobuko Yoshida \inst{8}\orcidID{0000-0002-3925-8557}
\and Daniel Zackon \inst{9}\orcidID{0009-0008-6153-2955}
}

\institute{
     IT University of Copenhagen, Copenhagen, Denmark \email{maca@itu.dk}, \email{dati@itu.dk}
\and University of Kent, Canterbury, United Kingdom \email{D.Castro-Perez@kent.ac.uk}
\and Royal Holloway, University of London, Egham, United Kingdom \email{Francisco.FerreiraRuiz@rhul.ac.uk}
\and University of Liverpool, Liverpool, United Kingdom \email{Lorenzo.Gheri@liverpool.ac.uk}
\and Technical University of Denmark, Kgs. Lyngby, Denmark \email{fkjac@dtu.dk}, \email{alcsc@dtu.dk}
\and Università degli Studi di Milano, Milan, Italy \email{momigliano@di.unimi.it}
\and Università di Camerino, Camerino, Italy \email{luca.padovani@unicam.it}
\and University of Oxford, Oxford, United Kingdom \email{martin.vassor@cs.ox.ac.uk}, \email{nobuko.yoshida@cs.ox.ac.uk}
\and McGill University, Montreal, Canada \email{daniel.zackon@mcgill.ca}
}

\authorrunning{M. Carbone et al.}

\maketitle

\begin{abstract}
  POPLMark and POPLMark Reloaded sparked a flurry of work on machine-checked proofs, and fostered the adoption of proof mechanisation in programming language research.
  Both challenges were purposely limited in scope, and their benchmark problems do not address concurrency-related issues.
  %
  We propose a new collection of benchmark problems focused on the difficulties that typically arise when mechanising formal models of concurrent and distributed programming languages, such as process calculi.
  Our benchmark problems address three key topics: linearity, scope extrusion, and coinductive reasoning.
  The goal of this new challenge is to clarify, compare, and advance the state of the art, fostering the adoption of proof mechanisation in future research on message-passing concurrency.

\keywords{Mechanisation \and Process calculi \and Benchmark \and Coinduction \and Scope extrusion \and Linearity}
\end{abstract}

% - Introduction
%   - POPLMark accelerated progress in mechanisation tools for PL
%     - POPLMark Reloaded tried to do the same for logical relations
%     - But they were deliberately limited in scope to not include process calculi
%   - We propose a new collection of benchmark problems for process calculi
%     - Why not concurrency in general? (after this point, only mention process calculi)
%     - Why is there a problem with process calculus mechanisations?
%     - Our experience says there is, and we will cite other experiences below
%   - We have noticed three key issues
%     - They are not covered by POPLMark
%     - Techniques are less settled than for binders
%     - Others also mention them as problems: cite and explain references briefly for each challenge topic
%   - Our goals are to determine the state of the art, develop best practices and tutorials, and identify improvements to proof assistant technology that would materially help the situation
\section{Introduction}

The POPLMark challenge~\cite{POPLMark}
played a pivotal role in advancing the field of proof assistants,
libraries, and best practices for programming language
mechanisation. By providing a shared framework for systematically
evaluating mechanisation techniques, it catalysed a significant
shift towards publications that include mechanised proofs within the programming language research community.

POPLMark Reloaded~\cite{POPLMarkReloaded} introduced a program akin to
the original POPLMark challenge, specifically targeting proofs related
to logical relations. While these initiatives had a narrow focus,
their authors recognized the importance of addressing broader topics
in the future, including coinduction and linearity.

In this spirit, we introduce a new collection of benchmark problems
specifically crafted to tackle common challenges encountered during
the mechanisation of formal models for concurrent and distributed
programming languages. Our focus extends particularly to process
calculi.
Concurrent calculi are notably subtle: For instance, it took nine
years before the incorrect proof of type safety in the seminal paper
by Honda et al.~\cite{Honda1998} was rectified in Yoshida \&
Vasconcelos’s subsequent work\cite{Yoshida2007}.
Similarly, many
key results in papers on session types have turned out
to be false later on~\cite{Gay2020,10.1145/3290343}, illustrating the
value of machine-checkable proofs.

While results about concurrent
formalisms have already been mechanised (as we will discuss further
below), our experience is that choosing appropriate mechanisation
techniques and tools remains a significant challenge and that the
trade-offs are not well understood.  This often leads researchers
toward a trial-and-error approach, resulting in sub-optimal solutions,
wasted mechanisation effort, and techniques and results that are hard
to reuse. For example, the authors in~\cite{Cruz-Filipe2021b} note that the high-level parts of
mechanised proofs closely resemble the informal ones, while the
main challenge lies in getting the foundations right.

Our benchmark problems (detailed in \cref{app:challenges}) consider
\emph{in isolation} three key aspects that frequently pose challenges
when mechanising concurrency theory: \emph{linearity}, \emph{scope
  extrusion}, and \emph{coinductive reasoning}.  Mechanisations must
often address several of these aspects at the same time; however, we
see the combination of techniques as a next
step%beyond the scope of this challenge
, as discussed further in \Cref{sec:going-beyond}.  Our benchmark
challenges are based on process calculi, as these provide a simple but
realistic showcase of common issues encountered during mechanisation
efforts.
% While the three highlighted aspects are not the sole sources of difficulties, they emerge in most mechanisations of concurrency theory.

We have begun collecting solutions to our challenges on our website:
%
\begin{center}
  \url{https://concurrentbenchmark.github.io/}
\end{center}
%
In the longer term, we expect to use the website for promoting best
practices, tutorials and guidelines derived from solutions to our
challenges.  We encourage anyone interested to try the challenges
using their favourite tools and techniques, and to send us their
solutions.

\section{Design of the Benchmark}\label{sec:design-discussion}
In this section, we outline the factors considered in designing the benchmark challenges.
We begin with some general remarks, then describe the individual design considerations for each challenge problem, and the evaluation criteria for solutions.

Similarly to the authors of the POPLMark challenges, we seek to
answer several questions:
\alb{Should we stress we are interested in the \textbf{meta-theory} of process calculi, not}
\begin{enumerate}[label=\textbf{(Q\arabic*)},leftmargin=10mm]
\item\label{item:rq1} What is the current state of the art in mechanising process calculi?
\item\label{item:rq2} Which techniques and best practices can be recommended when starting formalisation projects involving process calculi?
\item\label{item:rq3} Which improvements are needed to make mechanisation tools more user-friendly with regards to the issues faced when mechanising process calculi?
\end{enumerate}

To provide a framework in which to answer these questions, our benchmark is designed to satisfy three main goals:
\begin{enumerate}[label=\textbf{(G\arabic*)},leftmargin=10mm]
\item\label{item:goal-comperison-accessibility} To enable the comparison of
  proof mechanisation approaches, making the challenges accessible to
  mechanisation experts who may be unfamiliar with concurrency theory;

\item\label{item:goal-tutorials} To encourage the development of guidelines and
  tutorials demonstrating and comparing existing proof mechanisation
  techniques, libraries, and proof assistant features; and

\item\label{item:goal-reusability} To prioritise the exploration of mechanisation
  techniques that are reusable for future research.
\end{enumerate}
We also aim at strengthening the culture of mechanisation, by rallying the community to collaborate on exploring and developing new tools and techniques.

\todo{Refer to the future benchmark section about other things one could do, but they still all need the basics}
Still, these (and other) applications need the basic
techniques that our challenge problems emphasise:
Aligned with design goal~\ref{item:goal-reusability}, our problems are rooted in the fundamental meta-theory of process calculi, without necessitating the creation of novel theory. These problems centre around well-established theorems and results, showcasing interesting proof techniques that can be leveraged in future endeavours.
To achieve design goal \ref{item:goal-comperison-accessibility}, we have
formulated our challenge problems to explore the three aspects
(linearity, scope extrusion, coinduction) independently, so that they
may be solved individually and in any order; each problem is
reasonably small and easily understandable with basic knowledge of
textbook concurrency theory, process calculi, and type theory.  The
process calculus used in each challenge focuses on the features that
we want to emphasise, and omits all constructs (such as choices) that
would complicate the mechanisation without bringing additional
tangible benefits and insights.  The minimality and uniformity of the
calculi also allows us to target design goal
\ref{item:goal-tutorials}.

\subsubsection{Linearity.}
In mechanised meta-theory, addressing linearity means choosing an appropriate representation of the linear context.
The crux of the matter is \emph{context splitting}, as showcased by the typing rule for parallel composition.

\cite{Brady2017} notes that using algebraic effects is unwieldy when there are many communication channels and suggests an experimental approach using dependent uniqueness types.
\cite{Zalakain2019} notes that systems lacking linear types must simulate linearity, and that using a linearity predicate complicates congruences.
They had to limit themselves to an asymmetric structural precongruence to be able to prove subject reduction.


\alb{We need more details about what the challenge is}
For our challenge on linear reasoning we have chosen a type safety theorem
for a system using session types, combining as usual type preservation with absence of errors.
This allows us to introduce linear reasoning with few definitions.
With our objective focused on linearity, we forego channel delegation
and opt for a reduction approach rather than employing a transition
system semantics.  Inspired by Vasconcelos~\cite{Vasconcelos2012}, we
define a syntax where restriction binds two names, highlighting
their duality within the type system.  We have chosen a simple notion
of well-formedness, and do not  consider
deadlocks. While more sophisticated notions of well-formedness are
interesting, the proofs of these properties would distract from the
core linear aspects of the challenge.

While a linear context is perhaps best seen as a multi-set, most proof assistants have better support for list processing.
The latter representation is intuitive, but may require establishing a large number of technical lemmata that are orthogonal to the mathematics of the problem under study, say proving type preservation for session types.
In this space, several designs are possible: one can label occurrences of resources to constrain their usage (\eg~\cite{CicconeP20}) or \alb{something --- ask Francisco} as in \cite{Castro2020}.
Some authors instead impose a multiset structure over lists (\eg~\cite{ChaudhuriLR19,Danielsson12}); still, the effort required to develop the infrastructure is significant.

A different approach is to bypass the problem of context splitting by adopting familiar ideas from algorithmic linear type checking: this is known as \emph{typing with leftovers}, as exemplified in~\cite{DBLP:conf/forte/ZalakainD21}.
Whatever the choice, list-based encodings can be refined to be intrinsically-typed, if the proof assistant does support dependent types (see~\cite{Thiemann2019,CicconeP20,RouvoetPKV20}).

For something completely different, one could adopt a \emph{sub-structural} meta-logical framework: here the framework itself handles resource distribution, and users need only map their linear operations to the ones offered by the framework.
In fact, the linear function space will handle all context operations implicitly, including splitting and substitution.
One such framework is \emph{Celf}~\cite{Schack-Nielsen:IJCAR08} (see the encoding of session types in~\cite{Bock2016}).
Unfortunately, \emph{Celf} does not yet fully support the verification of meta-theoretic properties.
A compromise is the so called \emph{two-level} approach, where one encodes a sub-structural specification logic in a mainstream proof assistant and then uses that logic to state and prove linear properties (for a recent example, see~\cite{Felty:MSCS21}).

\alb{This needs some work: we do not describe the challenge at all and the citations are not the one I had in mind}
\subsubsection{Scope extrusion.}
The encoding of binding constructs has been extensively studied
w.r.t.\ the $\lambda$-calculi, but process calculi present additional
challenges.  $\pi$-calculi typically include several different binding
constructs: inputs bind a received name or value, recursive processes
bind recursion variables, and restrictions bind names.  The first two
act similarly to the binders known from $\lambda$-calculi, but
restrictions may be more challenging due to scope extrusion.

\cite{Maksimovic2015} does not have name restriction, thus dodging the challenge of scope extrusion.
\cite{Castro-Perez2021,Castro2020} note that popular presentations of session type systems are impossible to directly encode using intrinsically \(\alpha\)-convertible terms.
\cite{Castro2020} suggests using several binder constructs to solve this, but notes that this requires proving many variants of the same theorems.


\alb{We need more details about what the challenge is}
This is the challenge most closely related to the POPLMark challenge
since it concerns the properties of binders in restrictions and input.

The case study \cite{AmbalLS21} compares four approaches to encoding binders in Coq as a first step towards developing tools for working with higher-order process calculi.
They found that working directly with de Bruijn indices was easiest and shortest since the existing approaches developed for $\lambda$-calculus binders do not work well with scope extrusion.
Specifically, the locally nameless representation~\cite{Chargueraud2012} cannot avoid direct manipulation of de Bruijn indices when defining the semantics of scope extrusion; cofinite quantification provides no benefits when working under binders; nominal approaches~\cite{Pitts2003} require explicitly giving and validating sets of free names during scope extrusion; and higher-order abstract syntax (HOAS)~\cite{Pfenning1988} requires additional axioms to work with contexts.

Additionally, many proof assistants lack support for the proof principles required to easily work with scope extrusion, \ie, induction principles on functions to work with HOAS encodings.
Some proof assistants such as Abella~\cite{Baelde2014} support HOAS natively, but working with open terms as required to encode scope extrusion in these systems is generally very challenging~\cite{Momigliano2012}.

\subsubsection{Coinduction.}
Coinduction is fundamental to establish properties of processes that
may never terminate, which is a typical situation.
Common uses for coinduction are to define strong and weak versions of
bisimulation, barbed notions of bisimulation, and prove trace equivalence.
On the other hand, coinduction in proof assistants
is notoriously difficult and its representation
is non-uniform across developments.
E.g., \cite{Castro-Perez2021} notes that the most challenging
part of their mechanisation of multiparty session types
was the convoluted definition of infinite behaviours;
\cite{Pohjola2022} points out that the main difficulty
of the proofs concerning bisimulation lies
in dealing with existential quantifiers; and
\cite{Bengtson2016} wonders whether parametrised
coinduction as developed in Coq \cite{Hur2013} would
have helped their Isabelle mechanisation.


We base our challenge around bisimulation: a fundamental coinductively
established property for equating the
observed behaviour of potentially infinite processes. 
In this challenge, we study the notion of strong barbed bisimulation
(and bisimilarity, as in \cite{picalcbook})
over a fragment of the $\pi$-calculus and,
in particular, we focus on obtaining,
from strong barbed bisimilarity,
a congruence with respect to
parallel composition and substitution.

\alb{We need more details about what the challenge is}
\alb{I kinda object to ``seamlessly''. In Coq it's a mess, in Abella very very limited, sized types in Agda are shaky. I guess Isabelle is better}

Our coinduction challenge concerns strong barbed bisimulation and
congruence.% \alb{we motivate why strong vs weak, but not why barbed}
% \lore{Is it not enough to quote the Sangiorgi/Walker book
% and go by authority? :)}
Though weak barbed congruence is a more common behavioural equivalence
w.r.t.\ the \(\pi\)-calculus, here we prefer strong equivalences to avoid the
need to abstract over the number of internal transitions, thus
simplifying the theory.  We exclude delegation from the coinduction
challenge since it is orthogonal to our primary aim of exploring
coinductive proof techniques.

The introduction of strong barbed bisimulation is one of the first
steps when studying the (observable) behaviour of process calculi,
both in classical textbooks (e.g., \cite{picalcbook}) and in existing
mechanisations, where we see a variety of different formalisation
choices, sometimes ad-hoc for the specific result or development.

In fact, coinduction is widely supported in proof assistants.
Isabelle, Coq, and Abella seamlessly support coinductive definitions and reasoning.
Agda and Beluga support it through the convenient abstraction of co-patterns~\cite{Abel2013}.
Newer entrants to the field like the Lean proof assistant
are working on adding a notion of coinduction to their system~\cite{Avigad2019}%,Keizer2023}.
%
As with most features, proof assistants support
subtly different formalisms that enable different techniques and motivate this challenge.
Some systems provide more than one abstraction to use coinduction.
A good example of this are the parameterised coinduction~\cite{Hur2013}
and interaction tree~\cite{Xia2019} libraries for Coq.

\lore{Should we mention that Isabelle offers
  the command \texttt{coinduction\_upto}? Is it related?}
\todo{Explain the the point of the congruence theorem is that it should be ``easy'' once you have the other one}

While many authors rely on the native support for coinduction offered
by proof assistants
\cite{Honsell2001,Bengtson2016,Kahsai2008,Thiemann2019,Gay2020}, many
others prefer a more ``set-theoretic'' approach
\cite{Hirschkoff1997,Bengtson2009,Maksimovic2015,Pohjola2022}.
\alb{Actually, Honsell gives up with guarded coinduction and switches to Park's half way}

\subsubsection{Evaluation criteria.}
The idea behind our benchmark is to obtain evidence towards answering
questions \cref{item:rq1,item:rq2,item:rq3}. We are therefore
interested not only in the solutions, but also in the experience of
solving the challenges with the chosen approach.  Solutions to our
challenges should be compared on three axes:
\begin{enumerate}
\item Mechanisation overhead: the amount of manually written infrastructure and setup needed to express the definitions in the mechanisation;
\item Adequacy of the formal statements in the mechanisation: whether the proven theorems are easily recognisable as the theorems from the challenge; and
\item Cost of entry for the tools and techniques employed: the difficulty of learning to use the techniques.
\end{enumerate}
Solutions to our challenges do not need to strictly follow the definitions and lemmata set out in the problem text, but solutions which deviate from the original challenges must present more elaborate argumentation for their adequacy.


% - Going beyond the challenge problems
%   - We do not aim to be comprehensive, but the techniques should also be useful for other things
%   - Our challenges are independent, but they can also be combined
%   - Our challenges are basic textbook theory, so they can form the basis for many extensions
%   - Our aim is to see a future where ``the basics'' are as trivial in mechanisations as they are on paper
\section{Going beyond the challenge problems}\label{sec:going-beyond}
Similarly to POPLMark and its Reloaded counterpart, our challenges do not encompass all issues in the field.
Our benchmark is extendable in two aspects: one can further amalgamate and broaden the existing challenges, or tackle aspects not covered by the benchmark at all.
Any of these extensions will still need to handle at least one of the key aspects of our challenges.
\todo{Frederik: Does splitting this section up into these two categories still make sense? We could save some space by not doing so, and it is not obvious to me what exactly should be on each side of the split.}

\paragraph{Extending and combining the current challenges.}
We have aimed to reduce the overlap of the aspects (linearity, scope extrusion, coinduction) that our challenges involve.
Most concurrency theory research needs several of the aspects and often also other constructs that we have chosen not to cover for simplicity.
We suggest three extensions that would be useful for many applications:
\emph{(1)} adding choice constructs to both type and process levels,
\emph{(2)} introducing recursion and recursive types for typed systems, and
\emph{(3)} incorporating channel delegation in calculi lacking it.

\cite{DBLP:conf/forte/ZalakainD21} notes that the combination of linearity and infinite behaviour is difficult since it requires uniquely determining the resources consumed by a process.
\cite{Zalakain2019} notes that mixing replication and linearity seems to be very difficult without using explicit type contexts.

\paragraph{Addressing new calculi and new features.}
\todo{Mention the higher-order pi calculus? If so, also mention~\cite{Hirsch2022}}
Extending in this direction means proposing new challenges covering different features of process calculi, or different proof mechanisation features.
Some interesting aspects of message-passing calculi to be explored in further challenges could be multiparty session types~\cite{10.1145/2827695} and choreographies~\cite{Carbone2013}, as their meta-theory includes aspects -- \eg well-formedness conditions on global types, partiality of end-point projection function, \etc -- that we do not address here.
Also, one could easily design a challenge that goes beyond our coinduction challenge, by exploring different notions of bisimilarity (barbed, weak, \etc) and trace equivalence.
%
Coinduction may also play a relevant role in supporting recursive processes and
session types. Indeed, it is often the case that session type systems (and
logics) with recursive session types (and propositions) are naturally expressed
in \emph{infinitary form} by interpreting their typing (and proof) rules
coinductively~\cite{BaeldeDoumaneSaurin16,DerakhshanPfenning22,HornePadovani23}.

Like the POPLMark and POPLMark Reloaded challenges, our challenge is
not meant to be comprehensive: applications such as multiparty session
types~\cite{10.1145/2827695,10.1145/3290343},
choreographies~\cite{DBLP:journals/jar/CruzFilipeMP23}, conversation
types~\cite{DBLP:journals/tcs/CairesV10}, psi-calculi~\cite{lmcs:696},
or encodings between different
calculi~\cite{DBLP:journals/iandc/Gorla10, DBLP:conf/ecoop/ScalasDHY17} are not directly covered.\todo{Should we cite other/different encoding papers? Which ones are the ``standard'' citations?}

\todo{This is a list of interesting research problems, but they are not challenges per se. Trim down.}
On the mechanisation side, the present challenge proposes mechanising the proofs of theorems.
An interesting avenue to explore would be to take advantage of other proof assistant features like extracting certified implementations.
Additionally, while our current challenge is agnostic about proof automation, a more specific challenge could be designed with the objective of automating aspects of the proofs.

Ultimately, the current challenges can be extended in several worthwhile directions, and we look forward to a future when they indeed are.
We see the current challenge as setting the foundation for those future extensions.
It is our hope and aim that our challenges will move us closer to a future where the key basic proof techniques are as easy to mechanise as they are to write on paper.

\bibliographystyle{splncs04}
\bibliography{../references}

\clearpage
\appendix
\section{Challenges}\label{app:challenges}
\input{../challenges/challenges.tex}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
