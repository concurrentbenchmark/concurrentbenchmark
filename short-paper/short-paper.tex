\documentclass[runningheads]{llncs}
\synctex=1
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{mathpartir}
\usepackage{cite}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage[inline]{enumitem}
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[status=draft]{fixme}
\fxsetup{theme=color}
% \setlength{\textwidth}{12.4cm}
\setlength{\textheight}{19.9cm}

\input{../macros.tex}

\begin{document}

\title{The Concurrent Calculi Formalisation Benchmark}

\author{
     Marco Carbone \inst{1}\orcidID{0000-0001-9479-2632}
\and David Castro-Perez \inst{2}\orcidID{0000-0002-6939-4189}
\and Francisco Ferreira \inst{3}\orcidID{0000-0001-8494-7696}
\and Lorenzo Gheri \inst{4}\orcidID{0000-0002-3191-7722}
\and Frederik Krogsdal Jacobsen \inst{5}\orcidID{0000-0003-3651-8314}
\and Alberto Momigliano \inst{6}\orcidID{0000-0003-0942-4777}
\and Luca Padovani \inst{7}\orcidID{0000-0001-9097-1297}
\and Alceste Scalas \inst{5}\orcidID{0000-0002-1153-6164}
\and Dawit Tirore \inst{1}\orcidID{0000-0002-1997-5161}
\and Martin Vassor \inst{8}\orcidID{0000-0002-2057-0495}
\and Nobuko Yoshida \inst{8}\orcidID{0000-0002-3925-8557}
\and Daniel Zackon \inst{9}\orcidID{0009-0008-6153-2955}
}

\institute{
     IT University of Copenhagen, Copenhagen, Denmark \email{maca@itu.dk}, \email{dati@itu.dk}
\and University of Kent, Canterbury, United Kingdom \email{D.Castro-Perez@kent.ac.uk}
\and Royal Holloway, University of London, Egham, United Kingdom \email{Francisco.FerreiraRuiz@rhul.ac.uk}
\and University of Liverpool, Liverpool, United Kingdom \email{Lorenzo.Gheri@liverpool.ac.uk}
\and Technical University of Denmark, Kgs. Lyngby, Denmark \email{fkjac@dtu.dk}, \email{alcsc@dtu.dk}
\and Università degli Studi di Milano, Milan, Italy \email{momigliano@di.unimi.it}
\and Università di Camerino, Camerino, Italy \email{luca.padovani@unicam.it}
\and University of Oxford, Oxford, United Kingdom \email{martin.vassor@cs.ox.ac.uk}, \email{nobuko.yoshida@cs.ox.ac.uk}
\and McGill University, Montreal, Canada \email{daniel.zackon@mcgill.ca}
}

\authorrunning{M. Carbone et al.}

\maketitle

\begin{abstract}
  POPLMark and POPLMark Reloaded sparked a flurry of work on machine-checked proofs, and fostered the adoption of proof mechanisation in programming language research.
  Both challenges were purposely limited in scope, and their benchmark problems do not address concurrency-related issues.
  %
  We propose a new collection of benchmark problems focused on the difficulties that typically arise when mechanising formal models of concurrent and distributed programming languages, such as process calculi.
  Our benchmark problems address three key topics: linearity, scope extrusion, and coinductive reasoning.
  The goal of this new challenge is to clarify, compare, and advance the state of the art, fostering the adoption of proof mechanisation in future research on message-passing concurrency.

\keywords{Mechanisation \and Process calculi \and Benchmark \and Coinduction \and Scope extrusion \and Linearity}
\end{abstract}

% - Introduction
%   - POPLMark accelerated progress in mechanisation tools for PL
%     - POPLMark Reloaded tried to do the same for logical relations
%     - But they were deliberately limited in scope to not include process calculi
%   - We propose a new collection of benchmark problems for process calculi
%     - Why not concurrency in general? (after this point, only mention process calculi)
%     - Why is there a problem with process calculus mechanisations?
%     - Our experience says there is, and we will cite other experiences below
%   - We have noticed three key issues
%     - They are not covered by POPLMark
%     - Techniques are less settled than for binders
%     - Others also mention them as problems: cite and explain references briefly for each challenge topic
%   - Our goals are to determine the state of the art, develop best practices and tutorials, and identify improvements to proof assistant technology that would materially help the situation
\section{Introduction}

The POPLMark challenge~\cite{POPLMark}
played a pivotal role in advancing the field of proof assistants,
libraries, and best practices for programming language
mechanisation. By providing a shared framework for systematically
evaluating mechanisation techniques, it catalysed a significant
shift towards publications that include mechanised proofs within the programming language research community.
POPLMark Reloaded~\cite{POPLMarkReloaded} introduced a similar programme targeting proofs using logical relations.
These initiatives had a narrow focus, but their authors recognised the importance of addressing topics such as coinduction and linearity in the future.

In this spirit, we introduce a new collection of benchmark problems
specifically crafted to tackle common challenges encountered during
the mechanisation of formal models for concurrent and distributed
programming languages.
We particularly focus on process calculi.
Concurrent calculi are notably subtle: For instance, it took nine
years before the incorrect proof of type safety in the seminal paper
by Honda et al.~\cite{Honda1998} was rectified in Yoshida \&
Vasconcelos’s subsequent work\cite{Yoshida2007}.
Similarly, several key results in papers on session types have turned out
to be false later on, illustrating the
value of machine-checked proofs~\cite{Gay2020,10.1145/3290343}.

While results about concurrent
formalisms have already been mechanised (as we will discuss further
below), our experience is that choosing appropriate mechanisation
techniques and tools remains a significant challenge and that the
trade-offs are not well understood.  This often leads researchers
toward a trial-and-error approach, resulting in sub-optimal solutions,
wasted mechanisation effort, and techniques and results that are hard
to reuse. For example, the authors in~\cite{Cruz-Filipe2021b} note that the high-level parts of
mechanised proofs closely resemble the informal ones, while the
main challenge lies in getting the foundations right.

Our benchmark problems (detailed in \cref{app:challenges} and on our website) consider
\emph{in isolation} three key aspects that frequently pose challenges
when mechanising concurrency theory: \emph{linearity}, \emph{scope
  extrusion}, and \emph{coinductive reasoning}.  Mechanisations must
often address several of these aspects; however, we
see the combination of techniques as a next
step, as discussed in \cref{sec:going-beyond}.
Our benchmark challenges are based on process calculi, as these provide a simple but
realistic showcase of common issues encountered during mechanisation
efforts.

We have begun collecting solutions to our challenges on our website:
%
\begin{center}
  \url{https://concurrentbenchmark.github.io/}
\end{center}
%
In the future, we expect to use the website to promote best practices and tutorials derived from solutions to our challenges.
We encourage everyone to try the challenges using their favourite techniques, and to send us their solutions.

\section{The Benchmark and its Design}\label{sec:design-discussion}
\alb{Next paragraph needs rewriting}
In this section, we outline the factors considered in designing the benchmark challenges.
We begin with some general remarks, then describe the individual design considerations for each challenge problem, and the evaluation criteria for solutions.

Similarly to the authors of the POPLMark challenges, we seek to answer several questions:
\begin{enumerate}[label=\textbf{(Q\arabic*)},leftmargin=10mm]
\item\label{item:rq1} What is the current state of the art in tools for mechanising the meta-theory of process calculi?
\item\label{item:rq2} Which techniques and best practices can be recommended when starting new mechanisation projects involving process calculi?
\item\label{item:rq3} Which improvements are needed to make mechanisation tools more user-friendly with regards to the issues faced when mechanising process calculi?
\end{enumerate}
%
\todo{Frederik: I'm not sure this note is necessary}
Note that the challenges revolve around the mechanisation of the
\emph{meta-theory} of process calculi, rather than interpreters or
type checkers for the latter.

To provide a framework in which to answer these questions, our benchmark is designed to satisfy three main design goals:
\begin{enumerate}[label=\textbf{(G\arabic*)},leftmargin=10mm]
\item\label{item:goal-comperison-accessibility} To enable the comparison of
  proof mechanisation approaches by making the challenges accessible to
  mechanisation experts who may be unfamiliar with concurrency theory;

\item\label{item:goal-tutorials} To encourage the development of guidelines and
  tutorials demonstrating and comparing existing proof mechanisation
  techniques, libraries, and proof assistant features; and

\item\label{item:goal-reusability} To prioritise the exploration of mechanisation
  techniques that are reusable for future research.
\end{enumerate}
We also aim at strengthening the culture of mechanisation, by rallying the community to collaborate on exploring and developing new tools and techniques.

To achieve design goal \ref{item:goal-comperison-accessibility}, we have
formulated our challenge problems to explore the three aspects
(linearity, scope extrusion, coinduction) independently, so that they
may be solved individually and in any order; each problem is
reasonably small and easily understandable with basic knowledge of
textbook concurrency theory, process calculi, and type theory.  The
process calculus used in each challenge focuses on the features that
we want to emphasise, and omits all constructs (such as choices) that
would complicate the mechanisation without bringing additional
tangible insights.  The minimality and uniformity of the
calculi also allows us to target design goal
\ref{item:goal-tutorials}.
Aligned with design goal~\ref{item:goal-reusability}, our problems are rooted in the fundamental meta-theory of process calculi, without necessitating the creation of novel theory.
Our challenges centre around well-established theorems and results, showcasing interesting proof techniques that can be leveraged in many applications (as we will further discuss in \cref{sec:going-beyond}).

\subsubsection{Linearity.}

% \cite{Brady2017} notes that using algebraic effects is unwieldy when there are many communication channels and suggests an experimental approach using dependent uniqueness types.
% \cite{Zalakain2019} notes that systems lacking linear types must simulate linearity, and that using a linearity predicate complicates congruences.
% They had to limit themselves to an asymmetric structural precongruence to be able to prove subject reduction.



For our challenge on linear reasoning we have chosen a type safety
theorem for a system using session types, combining as usual subject reduction with absence of errors.  This allows us to introduce
linear reasoning with few definitions.  With this focus, we forgo
channel delegation: input and output receives/sends only values, which
do not include channels, hence the topology of the communication
network described by a process cannot change.  We do not allow
recursion or replication in the syntax, hence no infinite behaviours
can be expressed. In particular, we only model linear (as opposed to
shared) channels. Inspired by Vasconcelos~\cite{Vasconcelos2012}, we
define a syntax where restriction binds two names, highlighting their
duality within the type system. The latter enforces
 two invariants, namely that no endpoint is used simultaneously by parallel processes and
 that the two endpoints of the same session have dual types.
The operational semantics is formulated in term of reductions rather
than via a transition system, allowing implementors to explore
alternative ways to encode structural congruence\footnote{\alb{Should we mention this in challenge 2?}}.

The statement of subject reduction is the usual one, encompassing type
preservation for structural congruence. W.r.t.\ type safety, we have
chosen a simple notion of what an error is: well-typed processes are a
well-formed in the sense that they do not use endpoints in a non-dual
way.  While more sophisticated notions of well-formedness that
prevent \eg deadlocks are interesting, the proofs of those properties
would distract from the core linear aspects of the challenge.

In mechanised meta-theory, addressing linearity means choosing an
appropriate representation of the linear context.  The crux of the
matter is \emph{context splitting}, as showcased by the typing rule
for parallel composition.  While a linear context is perhaps best seen
as a multi-set, most proof assistants have better support for list
processing.  The latter representation is intuitive, but may require
establishing a large number of technical lemmata that are orthogonal
to the mathematics of the problem under study, in our case proving
type safety for session types.  In this space, several designs are
possible: one can label occurrences of resources to constrain their
usage (\eg~\cite{CicconeP20}); some authors instead impose a multiset
structure over lists
(\eg~\cite{Danielsson12,ChaudhuriLR19}). Alternatively, contexts can
be implemented as finite maps (as in \cite{Castro2020}), whose operations
are sensitive to a linear discipline. In all these cases, the effort
required to develop the infrastructure is significant.

A different approach is to bypass the problem of context splitting by adopting familiar ideas from algorithmic linear type checking: this is known as \emph{typing with leftovers}, as exemplified in~\cite{DBLP:conf/forte/ZalakainD21}.
Whatever the choice, list-based encodings can be refined to be intrinsically-typed, if the proof assistant supports dependent types (see~\cite{Thiemann2019,CicconeP20,RouvoetPKV20}).

A completely different approach is to adopt a \emph{sub-structural} meta-logical framework: here the framework itself handles resource distribution, and users need only map their linear operations to the ones offered by the framework.
The linear function space will then handle all context operations implicitly, including splitting and substitution.
One such framework is \emph{Celf}~\cite{Schack-Nielsen:IJCAR08} (see the encoding of session types in~\cite{Bock2016}).
Unfortunately, \emph{Celf} does not yet fully support the verification of meta-theoretic properties.
A compromise is the so called \emph{two-level} approach, where one encodes a sub-structural specification logic in a mainstream proof assistant and then uses that logic to state and prove linear properties (for a recent example, see~\cite{Felty:MSCS21}).

\subsubsection{Scope extrusion.}
\alb{This needs some work: we do not describe the challenge at all and the citations are not the one I had in mind}

The encoding of binding constructs has been extensively studied
w.r.t.\ the $\lambda$-calculi, but process calculi present additional
challenges.  $\pi$-calculi typically include several different binding
constructs: inputs bind a received name or value, recursive processes
bind recursion variables, and restrictions bind names.  The first two
act similarly to the binders known from $\lambda$-calculi, but
restrictions may be more challenging due to scope extrusion.

\cite{Maksimovic2015} does not have name restriction, thus dodging the challenge of scope extrusion.
\cite{Castro-Perez2021,Castro2020} note that popular presentations of session type systems are impossible to directly encode using intrinsically \(\alpha\)-convertible terms.
\cite{Castro2020} suggests using several binder constructs to solve this, but notes that this requires proving many variants of the same theorems.


\alb{We need more details about what the challenge is}
This is the challenge most closely related to the POPLMark challenge
since it concerns the properties of binders in restrictions and input.

The case study \cite{AmbalLS21} compares four approaches to encoding binders in Coq as a first step towards developing tools for working with higher-order process calculi.
They found that working directly with de Bruijn indices was easiest and shortest since the existing approaches developed for $\lambda$-calculus binders do not work well with scope extrusion.
Specifically, the locally nameless representation~\cite{Chargueraud2012} cannot avoid direct manipulation of de Bruijn indices when defining the semantics of scope extrusion; cofinite quantification provides no benefits when working under binders; nominal approaches~\cite{Pitts2003} require explicitly giving and validating sets of free names during scope extrusion; and higher-order abstract syntax (HOAS)~\cite{Pfenning1988} requires additional axioms to work with contexts.

Additionally, many proof assistants lack support for the proof principles required to easily work with scope extrusion, \ie, induction principles on functions to work with HOAS encodings.
Some proof assistants such as Abella  support HOAS natively, but working with open terms as required to encode scope extrusion in these systems is generally very challenging.

\subsubsection{Coinduction.}
Coinduction is fundamental to establish properties of processes that
may never terminate, which is a typical situation.
Common uses for coinduction are to define strong and weak versions of
bisimulation, barbed notions of bisimulation, and prove trace equivalence.
On the other hand, coinduction in proof assistants
is notoriously difficult and its representation
is non-uniform across developments.
E.g., \cite{Castro-Perez2021} notes that the most challenging
part of their mechanisation of multiparty session types
was the convoluted definition of infinite behaviours;
\cite{Pohjola2022} points out that the main difficulty
of the proofs concerning bisimulation lies
in dealing with existential quantifiers; and
\cite{Bengtson2016} wonders whether parametrised
coinduction as developed in Coq \cite{Hur2013} would
have helped their Isabelle mechanisation.


We base our challenge around bisimulation: a fundamental coinductively
established property for equating the
observed behaviour of potentially infinite processes. 
In this challenge, we study the notion of strong barbed bisimulation
(and bisimilarity, as in \cite{picalcbook})
over a fragment of the $\pi$-calculus and,
in particular, we focus on obtaining,
from strong barbed bisimilarity,
a congruence with respect to
parallel composition and substitution.

\alb{We need more details about what the challenge is}
\alb{I kinda object to ``seamlessly''. In Coq it's a mess, in Abella very very limited, sized types in Agda are shaky. I guess Isabelle is better}

Our coinduction challenge concerns strong barbed bisimulation and
congruence.% \alb{we motivate why strong vs weak, but not why barbed}
% \lore{Is it not enough to quote the Sangiorgi/Walker book
% and go by authority? :)}
Though weak barbed congruence is a more common behavioural equivalence
w.r.t.\ the \(\pi\)-calculus, here we prefer strong equivalences to avoid the
need to abstract over the number of internal transitions, thus
simplifying the theory.  We exclude delegation from the coinduction
challenge since it is orthogonal to our primary aim of exploring
coinductive proof techniques.

The introduction of strong barbed bisimulation is one of the first
steps when studying the (observable) behaviour of process calculi,
both in classical textbooks (e.g., \cite{picalcbook}) and in existing
mechanisations, where we see a variety of different formalisation
choices, sometimes ad-hoc for the specific result or development.

In fact, coinduction is widely supported in proof assistants.
Isabelle, Coq, and Abella seamlessly support coinductive definitions and reasoning.
Agda and Beluga support it through the convenient abstraction of co-patterns~\cite{Abel2013}.
Newer entrants to the field like the Lean proof assistant
are working on adding a notion of coinduction to their system~\cite{Avigad2019}%,Keizer2023}.
%
As with most features, proof assistants support
subtly different formalisms that enable different techniques and motivate this challenge.
Some systems provide more than one abstraction to use coinduction.
A good example of this are the parameterised coinduction~\cite{Hur2013}
and interaction tree~\cite{Xia2019} libraries for Coq.

\lore{Should we mention that Isabelle offers
  the command \texttt{coinduction\_upto}? Is it related?}
\todo{Explain the the point of the congruence theorem is that it should be ``easy'' once you have the other one}

While many authors rely on the native support for coinduction offered
by proof assistants
\cite{Honsell2001,Bengtson2016,Kahsai2008,Thiemann2019,Gay2020}, many
others prefer a more ``set-theoretic'' approach
\cite{Hirschkoff1997,Bengtson2009,Maksimovic2015,Pohjola2022}.
\alb{Actually, Honsell gives up with guarded coinduction and switches to Park's half way}

\subsubsection{Evaluation criteria.}
The idea behind our benchmark is to obtain evidence towards answering
questions \cref{item:rq1,item:rq2,item:rq3}. We are therefore
interested not only in the solutions, but also in the experience of
solving the challenges with the chosen approach.  Solutions to our
challenges should be compared on three axes:
\begin{enumerate}
\item Mechanisation overhead: the amount of manually written infrastructure and setup needed to express the definitions in the mechanisation;
\item Adequacy of the formal statements in the mechanisation: whether the proven theorems are easily recognisable as the theorems from the challenge; and
\item Cost of entry for the tools and techniques employed: the difficulty of learning to use the techniques.
\end{enumerate}
Solutions to our challenges do not need to strictly follow the definitions and lemmata set out in the problem text, but solutions which deviate from the original challenges must present more elaborate argumentation for their adequacy.


% - Going beyond the challenge problems
%   - We do not aim to be comprehensive, but the techniques should also be useful for other things
%   - Our challenges are independent, but they can also be combined
%   - Our challenges are basic textbook theory, so they can form the basis for many extensions
%   - Our aim is to see a future where ``the basics'' are as trivial in mechanisations as they are on paper

% We previously promise to discuss combinations of techniques and applications to more elaborate schemes
\section{Going Beyond the Challenge Problems}\label{sec:going-beyond}
Our benchmark challenges do not cover all issues in the field, but focus on the fundamental textbook aspects of linearity, scope extrusion, and coinduction.
Many mechanisations need to combine techniques to handle several of these aspects, and some may also need to handle aspects that are not covered by our benchmark challenges at all.

Some authors have already described the difficulties that combining techniques may induce in a mechanisation, e.g.~\cite{DBLP:conf/forte/ZalakainD21}.

Features that are in wide use, but not covered by our benchmark, include choice constructs and recursion.
Some interesting aspects of message-passing calculi include multiparty session types~\cite{10.1145/2827695} and choreographies~\cite{Carbone2013}, as their meta-theory includes aspects -- \eg well-formedness conditions on global types, partiality of end-point projection function, \etc -- that we do not address.
Other areas of interest include higher-order calculi~\cite{Hirsch2022}, conversation types~\cite{DBLP:journals/tcs/CairesV10}, psi-calculi~\cite{lmcs:696}, or encodings between different calculi~\cite{DBLP:journals/iandc/Gorla10, DBLP:conf/ecoop/ScalasDHY17}.
\todo{Should we cite other/different encoding papers? Which ones are the ``standard'' citations?}

For coinduction, one could explore different notions of process equivalence.

Coinduction may also play a relevant role in supporting recursive processes and session types.
Indeed, it is often the case that session type systems with recursive session types are naturally expressed in \emph{infinitary form} by interpreting their typing rules
coinductively~\cite{DerakhshanPfenning22,HornePadovani23}.

Finally, another interesting avenue to explore would be to take
advantage of certain proof assistant features: for example, extracting
certified implementations (as in \cite{Castro-Perez2021}) or exploiting
proof automation, such as the one offered by the \emph{Hammer}
protocol~\cite{BohmeN10,CzajkaK18}.

Ultimately, our challenges can be extended in several worthwhile directions, and we look forward to a future where they indeed are.
It is our hope and aim that our challenges will move us closer to a future where the key basic proof techniques for concurrent calculi are as easy to mechanise as they are to write on paper.

\bibliographystyle{splncs04}
\bibliography{../references}

\clearpage
\appendix
\section{Challenges}\label{app:challenges}
\input{../challenges/challenges.tex}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
